{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4pHNzGI51fq"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import os\n",
        "from pickle import dump, load\n",
        "import numpy as np\n",
        "from torchvision.models import resnet152\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "1if7Moxb6Opq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Function to load document file and read the contents inside the file into a string\n",
        "def load_doc(filename):\n",
        "    with open(filename, 'r') as f:\n",
        "        text = f.read()\n",
        "    return text\n",
        "\n",
        "# Function to create a dictionary that maps images with a list of 5 captions\n",
        "def create_descriptions(filename):\n",
        "    file = load_doc(filename)\n",
        "    captions = file.split('\\n')\n",
        "    descriptions = {}\n",
        "    for caption in captions[:-1]:\n",
        "        img, caption = caption.split('\\t')\n",
        "        if img[:-2] not in descriptions:\n",
        "            descriptions[img[:-2]] = [caption]\n",
        "        else:\n",
        "            descriptions[img[:-2]].append(caption)\n",
        "    return descriptions\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(captions):\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    for img, caps in captions.items():\n",
        "        for i, img_caption in enumerate(caps):\n",
        "            img_caption.replace(\"-\", \" \")\n",
        "            desc = img_caption.split()\n",
        "            # converts to lowercase\n",
        "            desc = [word.lower() for word in desc]\n",
        "            # remove punctuation from each token\n",
        "            desc = [word.translate(table) for word in desc]\n",
        "            # remove hanging 's and a\n",
        "            desc = [word for word in desc if len(word) > 1]\n",
        "            # remove tokens with numbers in them\n",
        "            desc = [word for word in desc if word.isalpha()]\n",
        "            # convert back to string\n",
        "            img_caption = ' '.join(desc)\n",
        "            captions[img][i] = img_caption\n",
        "    return captions\n",
        "\n",
        "# Function to create vocabulary\n",
        "def create_vocab(descriptions):\n",
        "    vocab = set()\n",
        "    for key in descriptions.keys():\n",
        "        [vocab.update(d.split()) for d in descriptions[key]]\n",
        "    return vocab\n",
        "\n",
        "# Function to save preprocessed descriptions into a file named descriptions.txt\n",
        "def save_descriptions(descriptions, filename):\n",
        "    lines = []\n",
        "    for key, desc_list in descriptions.items():\n",
        "        for desc in desc_list:\n",
        "            lines.append(key + '\\t' + desc)\n",
        "    data = \"\\n\".join(lines)\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(data)\n"
      ],
      "metadata": {
        "id": "V2SN0WHh6UuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set these path according to project folder in you system\n",
        "dataset_text = path+\"Flickr_8k_text\"\n",
        "dataset_images = path+\"Flicker8k_Dataset\"\n",
        "\n",
        "#we prepare our text data\n",
        "\n",
        "filename = dataset_text + \"/\" + \"Flickr8k.token.txt\"\n",
        "filename ='/content/drive/MyDrive/mi/Flickr8k_text/Flickr8k.token.txt'\n",
        "#loading the file that contains all data\n",
        "#mapping them into descriptions dictionary img to 5 captions\n",
        "descriptions = all_img_captions(filename)\n",
        "print(\"Length of descriptions =\" ,len(descriptions))\n",
        "\n",
        "#cleaning the descriptions\n",
        "clean_descriptions = cleaning_text(descriptions)\n",
        "\n",
        "#building vocabulary\n",
        "vocabulary = text_vocabulary(clean_descriptions)\n",
        "print(\"Length of vocabulary = \", len(vocabulary))\n",
        "\n",
        "#saving each description to file\n",
        "\n",
        "save_descriptions(clean_descriptions, \"descriptions.txt\")\n",
        "def load_doc(filename):\n",
        "# Opening the file as read only\n",
        "with open(filename, 'r') as file:\n",
        "text = file.read()\n",
        "return text\n",
        "\n",
        "This function will create a descriptions dictionary that maps images with a list of 5 captions\n",
        "def all_img_captions(filename):\n",
        "file = load_doc(filename)\n",
        "captions = file.split('\\n')\n",
        "descriptions = {}\n",
        "for caption in captions[:-1]:\n",
        "img, caption = caption.split('\\t')\n",
        "if img[:-2] not in descriptions:\n",
        "descriptions[img[:-2]] = [caption]\n",
        "else:\n",
        "descriptions[img[:-2]].append(caption)\n",
        "return descriptions\n",
        "\n",
        "Data cleaning- lower casing, removing punctuations and words containing numbers\n",
        "def cleaning_text(captions):\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "for img, caps in captions.items():\n",
        "for i, img_caption in enumerate(caps):\n",
        "      img_caption.replace(\"-\", \" \")\n",
        "      desc = img_caption.split()\n",
        "\n",
        "      # converts to lowercase\n",
        "      desc = [word.lower() for word in desc]\n",
        "      # remove punctuation from each token\n",
        "      desc = [word.translate(table) for word in desc]\n",
        "      # remove hanging 's and a \n",
        "      desc = [word for word in desc if(len(word) > 1)]\n",
        "      # remove tokens with numbers in them\n",
        "      desc = [word for word in desc if(word.isalpha())]\n",
        "      # convert back to string\n",
        "\n",
        "      img_caption = ' '.join(desc)\n",
        "      captions[img][i] = img_caption\n",
        "return captions\n",
        "\n",
        "def text_vocabulary(descriptions):\n",
        "# build vocabulary of all unique words\n",
        "vocab = set()\n",
        "\n",
        "for key in descriptions.keys():\n",
        "    [vocab.update(d.split()) for d in descriptions[key]]\n",
        "\n",
        "return vocab"
      ],
      "metadata": {
        "id": "r0cc_QA86jJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(directory):\n",
        "    model = models.xception(pretrained=True)\n",
        "    model.eval()\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((299,299)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    features = {}\n",
        "    for img in tqdm(os.listdir(directory)):\n",
        "        filename = directory + \"/\" + img\n",
        "        image = Image.open(filename).convert('RGB')\n",
        "        image = transform(image)\n",
        "        image = image.unsqueeze(0)\n",
        "        if torch.cuda.is_available():\n",
        "            image = image.cuda()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            feature = model(image)\n",
        "        feature = feature.cpu().numpy().squeeze()\n",
        "        features[img] = feature\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "Jouoj57t7Nbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This will load the text file in a string and will return the list of image names.\n",
        "def load_photos(filename):\n",
        "    file = load_doc(filename)\n",
        "    photos = file.split(\"\\n\")[:-1]\n",
        "    return photos\n",
        "\n",
        "'''\n",
        "This function will create a dictionary that contains captions for each photo from the list of photos.\n",
        " We also append the <start> and <end> identifier for each caption.\n",
        "  We need this so that our LSTM model can identify the starting and ending of the caption.\n",
        "'''\n",
        "def load_clean_descriptions(filename, photos): \n",
        "    #loading clean_descriptions\n",
        "    file = load_doc(filename)\n",
        "    descriptions = {}\n",
        "    for line in file.split(\"\\n\"):\n",
        "\n",
        "        words = line.split()\n",
        "        if len(words)<1 :\n",
        "            continue\n",
        "\n",
        "        image, image_caption = words[0], words[1:]\n",
        "\n",
        "        if image in photos:\n",
        "            if image not in descriptions:\n",
        "                descriptions[image] = []\n",
        "            desc = '<start> ' + \" \".join(image_caption) + ' <end>'\n",
        "            descriptions[image].append(desc)\n",
        "\n",
        "    return descriptions\n",
        "\n",
        "##  This function will return a dictionary for image names and their feature vector which we have previously extracted from the Xception model.\n",
        "def load_features(photos):\n",
        "    #loading all features\n",
        "    all_features = load(open(\"/content/drive/MyDrive/mi/new/features.p\",\"rb\"))\n",
        "    #selecting only needed features\n",
        "    features = {k:all_features[k] for k in photos}\n",
        "    return features\n",
        "\n"
      ],
      "metadata": {
        "id": "ESRjP7M67bCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filename = dataset_text + \"/\" + \"Flickr_8k.trainImages.txt\"\n",
        "filename = '/content/drive/MyDrive/mi/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "\n",
        "#train = loading_data(filename)\n",
        "train_imgs = load_photos(filename)\n",
        "train_descriptions = load_clean_descriptions(\"/content/drive/MyDrive/mi/new/descriptions.txt\", train_imgs)\n",
        "train_features = load_features(train_imgs)"
      ],
      "metadata": {
        "id": "pxfdA_L577B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dict_to_list(descriptions):\n",
        "    all_desc = []\n",
        "    for key in descriptions.keys():\n",
        "        [all_desc.append(d) for d in descriptions[key]]\n",
        "    return all_desc\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "def create_tokenizer(descriptions):\n",
        "    desc_list = dict_to_list(descriptions)\n",
        "    tokenizer = get_tokenizer('basic_english')\n",
        "    tokenizer.fit_on_texts(desc_list)\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "7eF8Lc6b7-WX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def max_length(descriptions):\n",
        "    desc_list = dict_to_list(descriptions)\n",
        "    return max(len(d.split()) for d in desc_list)"
      ],
      "metadata": {
        "id": "bSCe4_5J8HBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max_length(descriptions)\n",
        "max_length"
      ],
      "metadata": {
        "id": "oprDcoMW8KvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# custom dataset class\n",
        "class ImageCaptionDataset(Dataset):\n",
        "    def __init__(self, descriptions, features, tokenizer, max_length):\n",
        "        self.descriptions = descriptions\n",
        "        self.features = features\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.descriptions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        key = list(self.descriptions.keys())[idx]\n",
        "        # retrieve photo features\n",
        "        feature = torch.Tensor(self.features[key][0])\n",
        "        input_image, input_sequence, output_word = self.create_sequences(self.tokenizer, self.max_length, self.descriptions[key], feature)\n",
        "        return input_image, input_sequence, output_word\n",
        "\n",
        "    def create_sequences(self, tokenizer, max_length, desc_list, feature):\n",
        "        X1, X2, y = [], [], []\n",
        "        # walk through each description for the image\n",
        "        for desc in desc_list:\n",
        "            # encode the sequence\n",
        "            seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "            # split one sequence into multiple X,y pairs\n",
        "            for i in range(1, len(seq)):\n",
        "                # split into input and output pair\n",
        "                in_seq, out_seq = seq[:i], seq[i]\n",
        "                # pad input sequence\n",
        "                in_seq = nn.utils.rnn.pad_sequence([torch.LongTensor(in_seq)], batch_first=True, padding_value=0)\n",
        "                # encode output sequence\n",
        "                out_seq = torch.zeros(self.vocab_size)\n",
        "                out_seq[out_seq == 0] = -1e9 # setting the probability of padding token to be very low\n",
        "                out_seq[out_seq == 0] = 0 # setting the probability of all other tokens to be zero\n",
        "                out_seq[out_seq == out_seq[out_seq != 0][out_seq[out_seq != 0].argmax()]] = 1 # setting the probability of the actual output token to be one\n",
        "                # store\n",
        "                X1.append(feature)\n",
        "                X2.append(in_seq.squeeze(0))\n",
        "                y.append(out_seq)\n",
        "        return torch.stack(X1), torch.stack(X2), torch.stack(y)\n",
        "\n",
        "# dataloader for the dataset\n",
        "def get_loader(descriptions, features, tokenizer, max_length, batch_size, shuffle=True, num_workers=2):\n",
        "    dataset = ImageCaptionDataset(descriptions, features, tokenizer, max_length)\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
        "    return dataloader\n"
      ],
      "metadata": {
        "id": "ysPMXR5P8Mrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#You can check the shape of the input and output for your model\n",
        "[a,b],c = next(data_generator(train_descriptions, features, tokenizer, max_length))\n",
        "a.shape, b.shape, c.shape\n",
        "# ((47, 2048), (47, 32), (47, 7577))"
      ],
      "metadata": {
        "id": "u7ChLa_c8iI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CaptioningModel(nn.Module):\n",
        "def init(self, vocab_size, max_length):\n",
        "    super(CaptioningModel, self).init()\n",
        "    # features from the CNN model squeezed from 2048 to 256 nodes\n",
        "    self.fe1 = nn.Dropout(0.5)\n",
        "    self.fe2 = nn.Linear(2048, 256)\n",
        "    # LSTM sequence model\n",
        "    self.se1 = nn.Embedding(vocab_size, 256)\n",
        "    self.se2 = nn.Dropout(0.5)\n",
        "    self.se3 = nn.LSTM(256, 256, batch_first=True)\n",
        "    # Merging both models\n",
        "    self.decoder1 = nn.Linear(256, 256)\n",
        "    self.decoder2 = nn.Linear(256, vocab_size)\n",
        "def forward(self, img_features, captions):\n",
        "    img_features = self.fe1(img_features)\n",
        "    img_features = self.fe2(img_features)\n",
        "    captions = self.se1(captions)\n",
        "    captions = self.se2(captions)\n",
        "    captions, _ = self.se3(captions)\n",
        "    decoder_input = img_features + captions\n",
        "    decoder_input = self.decoder1(decoder_input)\n",
        "    decoder_input = nn.functional.relu(decoder_input)\n",
        "    outputs = self.decoder2(decoder_input)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "model = CaptioningModel(vocab_size, max_length)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "SX_y9yJv8kES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train our model\n",
        "print('Dataset: ', len(train_imgs))\n",
        "print('Descriptions: train=', len(train_descriptions))\n",
        "print('Photos: train=', len(train_features))\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Description Length: ', max_length)\n",
        "\n",
        "model = define_model(vocab_size, max_length)\n",
        "epochs = 10\n",
        "steps = len(train_descriptions)\n",
        "# making a directory models to save our models\n",
        "os.mkdir(\"models\")\n",
        "for i in range(epochs):\n",
        "    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
        "    model.fit_generator(generator, epochs=1, steps_per_epoch= steps, verbose=1)\n",
        "    model.save(\"models/model_\" + str(i) + \".h5\")"
      ],
      "metadata": {
        "id": "jaGu_JZS89YP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "\n",
        "\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument('-i', '--image', required=False, help=\"Image Path\")\n",
        "# args = vars(ap.parse_args())\n",
        "# img_path = args['image']\n",
        "\n",
        "\n",
        "img_path = '/content/drive/MyDrive/mi/Flicker8k_Dataset/10815824_2997e03d76.jpg'\n",
        "\n",
        "#extractsfeatures of the test image\n",
        "\n",
        "def extract_features(filename, model):\n",
        "        try:\n",
        "            image = Image.open(filename)\n",
        "\n",
        "        except:\n",
        "            print(\"ERROR: Couldn't open image! Make sure the image path and extension is correct\")\n",
        "        image = image.resize((299,299))\n",
        "        image = np.array(image)\n",
        "        # for images that has 4 channels, we convert them into 3 channels\n",
        "        if image.shape[2] == 4: \n",
        "            image = image[..., :3]\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "        image = image/127.5\n",
        "        image = image - 1.0\n",
        "        feature = model.predict(image)\n",
        "        return feature\n",
        "\n",
        "def word_for_id(integer, tokenizer):\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "      if index == integer:\n",
        "          return word\n",
        "  return None\n",
        "\n",
        "# generates the caption for the given image\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "    in_text = 'start'\n",
        "    for i in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        pred = model.predict([photo,sequence], verbose=0)\n",
        "        pred = np.argmax(pred)\n",
        "        word = word_for_id(pred, tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        if word == 'end':\n",
        "            break\n",
        "    return in_text\n",
        "\n",
        "\n",
        "#path = 'Flicker8k_Dataset/111537222_07e56d5a30.jpg'\n",
        "max_length = 32\n",
        "tokenizer = load(open(\"/content/drive/MyDrive/mi/new/tokenizer.p\",\"rb\"))\n",
        "model = load_model('/content/drive/MyDrive/mi/new/model_9.h5')\n",
        "xception_model = Xception(include_top=False, pooling=\"avg\")\n",
        "\n",
        "photo = extract_features(img_path, xception_model)\n",
        "img = Image.open(img_path)\n",
        "\n",
        "description = generate_desc(model, tokenizer, photo, max_length)\n",
        "print(\"\\n\\n\")\n",
        "print(description)\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "_HVxLfpX9AAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = '/content/drive/MyDrive/mi/Flicker8k_Dataset/241345522_c3c266a02a.jpg'\n",
        "image = Image.open(img_path)\n",
        "\n",
        "photo = extract_features(img_path, xception_model)\n",
        "img = Image.open(img_path)\n",
        "\n",
        "description = generate_desc(model, tokenizer, photo, max_length)\n",
        "print(\"\\n\\n\")\n",
        "print(description)\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "rEqudOEv9Emq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = '/content/drive/MyDrive/mi/Flicker8k_Dataset/481827288_a688be7913.jpg'\n",
        "image = Image.open(img_path)\n",
        "\n",
        "photo = extract_features(img_path, xception_model)\n",
        "img = Image.open(img_path)\n",
        "\n",
        "description = generate_desc(model, tokenizer, photo, max_length)\n",
        "print(\"\\n\\n\")\n",
        "print(description)\n",
        "plt.imshow(img)\n",
        "\n"
      ],
      "metadata": {
        "id": "1vlQozC0CF5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = '/content/drive/MyDrive/mi/Flicker8k_Dataset/2511019188_ca71775f2d.jpg'\n",
        "image = Image.open(img_path)\n",
        "\n",
        "photo = extract_features(img_path, xception_model)\n",
        "img = Image.open(img_path)\n",
        "\n",
        "description = generate_desc(model, tokenizer, photo, max_length)\n",
        "print(\"\\n\\n\")\n",
        "print(description)\n",
        "plt.imshow(img)\n"
      ],
      "metadata": {
        "id": "ZBhvdSHXCH_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_path = '/content/drive/MyDrive/mi/Flicker8k_Dataset/3150742439_b8a352e1e0.jpg'\n",
        "image = Image.open(img_path)\n",
        "\n",
        "photo = extract_features(img_path, xception_model)\n",
        "img = Image.open(img_path)\n",
        "\n",
        "description = generate_desc(model, tokenizer, photo, max_length)\n",
        "print(\"\\n\\n\")\n",
        "print(description)\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "Lvy-qZH3CJ7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creates a dictionary of all test images along with their reference captions for calculating bleu score\n",
        "def load_clean_descriptions2(filename, photos): \n",
        "    #loading clean_descriptions\n",
        "    file = load_doc(filename)\n",
        "    descriptions = {}\n",
        "    for line in file.split(\"\\n\"):\n",
        "\n",
        "        words = line.split()\n",
        "        if len(words)<1 :\n",
        "            continue\n",
        "\n",
        "        image, image_caption = words[0], words[1:]\n",
        "\n",
        "        if image in photos:\n",
        "            if image not in descriptions:\n",
        "                descriptions[image] = []\n",
        "            desc = \" \".join(image_caption)\n",
        "            # desc=image_caption\n",
        "            descriptions[image].append(desc)\n",
        "\n",
        "    return descriptions"
      ],
      "metadata": {
        "id": "s2-XBao7CLzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filename = dataset_text + \"/\" + \"Flickr_8k.testImages.txt\"\n",
        "filename = '/content/drive/MyDrive/mi/Flickr8k_text/Flickr_8k.testImages.txt'\n",
        "\n",
        "#test = loading_data(filename)\n",
        "test_imgs = load_photos(filename)\n",
        "# test_descriptions = all_img_captions(\"/content/drive/MyDrive/mi/Flickr8k_text/Flickr8k.token.txt\")\n",
        "test_descriptions = load_clean_descriptions2(\"/content/drive/MyDrive/mi/Flickr8k_text/Flickr8k.token2.txt\", test_imgs)\n",
        "# train_features = load_features(train_imgs)"
      ],
      "metadata": {
        "id": "-nY9TZmXCPFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # evaluate the skill of the model using bleu score\n",
        "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
        "\tactual, predicted = list(), list()\n",
        "\t# step over the whole set\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\t# generate description\n",
        "\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
        "\t\t# store actual and predicted\n",
        "\t\treferences = [d.split() for d in desc_list]\n",
        "\t\tactual.append(references)\n",
        "\t\tpredicted.append(yhat.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % nltk.translate.bleu_score.corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % nltk.translate.bleu_score.corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % nltk.translate.bleu_score.corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % nltk.translate.bleu_score.corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "metadata": {
        "id": "3qdVRKmtCV13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates a disctionaly photos haveing images and their extracted fesautes\n",
        "img_path = '/content/drive/MyDrive/mi/Flicker8k_Dataset/'\n",
        "photos={}\n",
        "for i in range(len(test_imgs)):\n",
        "  img_path+=test_imgs[i]\n",
        "  # print(img_path)\n",
        "  # image = Image.open(img_path)\n",
        "  photo = extract_features(img_path, xception_model)\n",
        "  photos[test_imgs[i]] = photo\n",
        "  print(i)\n",
        "  img_path = '/content/drive/MyDrive/mi/Flicker8k_Dataset/'"
      ],
      "metadata": {
        "id": "HZRsyL_XCWcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model, test_descriptions, photos, tokenizer, max_length)"
      ],
      "metadata": {
        "id": "Druuo1h6CYTm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}